{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smomb/BaThesis_ImperviousCD/blob/main/BachelorImperviosCD_Holub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1SqTAdqe5JMD"
      },
      "outputs": [],
      "source": [
        "# Importing libraries/modules.\n",
        "\n",
        "import ee\n",
        "import geemap.core\n",
        "import geemap\n",
        "import os\n",
        "\n",
        "!pip install folium geopandas pandas matplotlib\n",
        "import folium\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pprint\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnTDsX7S5I2P",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#GEE authentication and initialization\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='bscthesischangedetection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBZYd_LHFGsv"
      },
      "outputs": [],
      "source": [
        "# Loading study area shape.\n",
        "\n",
        "catchment = ee.FeatureCollection('projects/bscthesischangedetection/assets/catchment_wienfluss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE1NLBXAGIAA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fetching and visualize data.\n",
        "\n",
        "map_center = catchment.geometry().centroid().coordinates().getInfo()[::-1]\n",
        "m = geemap.Map(center=map_center, zoom=9)\n",
        "m.addLayer(catchment, {}, 'Catchment')\n",
        "\n",
        "# Visualization parameters\n",
        "vis_tm = {'bands': ['SR_B3','SR_B2','SR_B1'], 'min': 0, 'max': 12000}\n",
        "vis_ol = {'bands': ['SR_B4','SR_B3','SR_B2'], 'min': 0, 'max': 12000}\n",
        "\n",
        "#Dataset 1: 1984 (Landsat 5 TM)\n",
        "image1_1 = ee.Image('LANDSAT/LT05/C02/T1_L2/LT05_190026_19840805')\n",
        "image1_1_clipped = image1_1.clip(catchment)\n",
        "m.addLayer(image1_1_clipped, vis_tm, 'scene_1_1984')\n",
        "\n",
        "#Dataset 2: 1997 (Landsat 5 TM)\n",
        "image1_2 = ee.Image('LANDSAT/LT05/C02/T1_L2/LT05_190026_19970825')\n",
        "image1_2_clipped = image1_2.clip(catchment)\n",
        "m.addLayer(image1_2_clipped, vis_tm, 'scene_1_1997')\n",
        "\n",
        "#Dataset 3: 2013 (Landsat 8 OLI)\n",
        "image1_3 = ee.Image('LANDSAT/LC08/C02/T1_L2/LC08_190026_20130805')\n",
        "image1_3_clipped = image1_3.clip(catchment)\n",
        "m.addLayer(image1_3_clipped, vis_ol, 'scene_1_2013')\n",
        "\n",
        "#Dataset 4: 2024 (Landsat 9 OLI-2)\n",
        "image1_4 = ee.Image('LANDSAT/LC09/C02/T1_L2/LC09_190026_20240811')\n",
        "image1_4_clipped = image1_4.clip(catchment)\n",
        "m.addLayer(image1_4_clipped, vis_ol, 'scene_1_2024')\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA2s-G-GHayl"
      },
      "outputs": [],
      "source": [
        "# Fetch and print metadata for each selected and clipped image.\n",
        "\n",
        "image1_metadata = {image1_1_clipped.get('DATE_ACQUIRED').getInfo(),\n",
        "                   image1_1_clipped.get('CLOUD_COVER').getInfo(),\n",
        "                   image1_1_clipped.get('system:id').getInfo()}\n",
        "pprint.pprint(image1_metadata)\n",
        "\n",
        "image2_metadata = {image1_2_clipped.get('DATE_ACQUIRED').getInfo(),\n",
        "                   image1_2_clipped.get('CLOUD_COVER').getInfo(),\n",
        "                   image1_2_clipped.get('system:id').getInfo()}\n",
        "pprint.pprint(image2_metadata)\n",
        "\n",
        "image3_metadata = {image1_3_clipped.get('DATE_ACQUIRED').getInfo(),\n",
        "                   image1_3_clipped.get('CLOUD_COVER').getInfo(),\n",
        "                   image1_3_clipped.get('system:id').getInfo()}\n",
        "pprint.pprint(image3_metadata)\n",
        "\n",
        "image4_metadata = {image1_4_clipped.get('DATE_ACQUIRED').getInfo(),\n",
        "                   image1_4_clipped.get('CLOUD_COVER').getInfo(),\n",
        "                   image1_4_clipped.get('system:id').getInfo()}\n",
        "pprint.pprint(image4_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z7KBZgBiONL"
      },
      "outputs": [],
      "source": [
        "#Apply Normalized Difference Water Index. Examine histogram for manual thresholding of water features from non-water features.\n",
        "\n",
        "#Using Modified Normalized Difference Water Index (mNDWI) instead of NDWI due to the large degree of urbanization in the study area.\n",
        "\n",
        "# 1984 (Landsat 5 TM): Green = SR_B2, SWIR1 = SR_B5\n",
        "mndwi_1 = image1_1_clipped.expression(\n",
        "    '(g - s) / (g + s)',\n",
        "    {'g': image1_1_clipped.select('SR_B2'),\n",
        "        's': image1_1_clipped.select('SR_B5')\n",
        "    }).rename('mndwi')\n",
        "\n",
        "# 1997 (Landsat 5 TM): same as previous\n",
        "mndwi_2 = image1_2_clipped.expression(\n",
        "    '(g - s) / (g + s)',\n",
        "    {'g': image1_2_clipped.select('SR_B2'),\n",
        "        's': image1_2_clipped.select('SR_B5')\n",
        "    }).rename('mndwi')\n",
        "\n",
        "# 2013 (Landsat 8): Green = SR_B3, SWIR1 = SR_B6\n",
        "mndwi_3 = image1_3_clipped.expression(\n",
        "    '(g - s) / (g + s)',\n",
        "    {'g': image1_3_clipped.select('SR_B3'),\n",
        "        's': image1_3_clipped.select('SR_B6')\n",
        "    }).rename('mndwi')\n",
        "\n",
        "# 2024 (Landsat 9): same bands as Landsat 8\n",
        "mndwi_4 = image1_4_clipped.expression(\n",
        "    '(g - s) / (g + s)',\n",
        "    {'g': image1_4_clipped.select('SR_B3'),\n",
        "        's': image1_4_clipped.select('SR_B6')\n",
        "    }).rename('mndwi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdcgxuG-_2Mv"
      },
      "outputs": [],
      "source": [
        "# Get the mndwi values as a NumPy array and then access the values using the band name.\n",
        "mndwi_1_np = np.array(mndwi_1.reduceRegion(\n",
        "    reducer=ee.Reducer.toList(),\n",
        "    geometry=mndwi_1.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e13\n",
        ").getInfo()['mndwi'])\n",
        "\n",
        "# Creating a histogram using matplotlib.pyplot\n",
        "plt.hist(mndwi_1_np, bins=50)\n",
        "plt.title(\"mNDWI Histogram - Image 1 1984\")\n",
        "plt.xlabel(\"mNDWI Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dJvlKz9Aie1"
      },
      "outputs": [],
      "source": [
        "# Repeating this for all images.\n",
        "mndwi_2_np = np.array(mndwi_2.reduceRegion(\n",
        "    reducer=ee.Reducer.toList(),\n",
        "    geometry=mndwi_2.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e13\n",
        ").getInfo()['mndwi'])\n",
        "plt.title(\"mNDWI Histogram - Image 2 1997\")\n",
        "plt.xlabel(\"mNDWI Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDAx83_eAjzF"
      },
      "outputs": [],
      "source": [
        "mndwi_3_np = np.array(mndwi_3.reduceRegion(\n",
        "    reducer=ee.Reducer.toList(),\n",
        "    geometry=mndwi_3.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e13\n",
        ").getInfo()['mndwi'])\n",
        "plt.hist(mndwi_3_np, bins=50)\n",
        "plt.title(\"mNDWI Histogram - Image 3 2013\")\n",
        "plt.xlabel(\"mNDWI Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKzwgnchAkfo"
      },
      "outputs": [],
      "source": [
        "mndwi_4_np = np.array(mndwi_4.reduceRegion(\n",
        "    reducer=ee.Reducer.toList(),\n",
        "    geometry=mndwi_4.geometry(),\n",
        "    scale=30,\n",
        "    maxPixels=1e13\n",
        ").getInfo()['mndwi'])\n",
        "plt.hist(mndwi_4_np, bins=50)\n",
        "plt.title(\"mNDWI Histogram - 2024\")\n",
        "plt.xlabel(\"mNDWI Value\")\n",
        "plt.ylabel(\"Pixel Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkdjmSGlBVxZ"
      },
      "outputs": [],
      "source": [
        "# Manual threshholding. Assuming 0.0 for all datasets due to the small amount of values over 0, likely being consistent with actual water areas present in the images.\n",
        "mndwi_threshhold_1 = 0.0\n",
        "mndwi_threshhold_2 = 0.0\n",
        "mndwi_threshhold_3 = 0.0\n",
        "mndwi_threshhold_4 = 0.0\n",
        "\n",
        "# Masking the water pixels with the previously defined threshhold.\n",
        "water_mask_1 = mndwi_1.gt(mndwi_threshhold_1)\n",
        "water_mask_2 = mndwi_2.gt(mndwi_threshhold_2)\n",
        "water_mask_3 = mndwi_3.gt(mndwi_threshhold_3)\n",
        "water_mask_4 = mndwi_4.gt(mndwi_threshhold_4)\n",
        "\n",
        "image1_1_clipped = image1_1_clipped.updateMask(water_mask_1.Not())\n",
        "image1_2_clipped = image1_2_clipped.updateMask(water_mask_2.Not())\n",
        "image1_3_clipped = image1_3_clipped.updateMask(water_mask_3.Not())\n",
        "image1_4_clipped = image1_4_clipped.updateMask(water_mask_4.Not())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tgL404eiUEI"
      },
      "outputs": [],
      "source": [
        "# Cloud masking and Quality Assurance-based pixel masking to eliminate clouds, cloud shadows, and unused pixels from the mosaics, following Landsat documentation.\n",
        "\n",
        "qa = image1_1_clipped.select('QA_PIXEL')\n",
        "cloud_mask = qa.bitwiseAnd(1 << 0).eq(0)   # bit 0: Fill, keep if 0 (not fill)\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 1).eq(0))  # bit 1: Dilated Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 2).eq(0))  # bit 2: Unused Pixel\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 3).eq(0))  # bit 3: Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 4).eq(0))  # bit 4: Cloud Shadow\n",
        "image1_1_clipped = image1_1_clipped.updateMask(cloud_mask)\n",
        "\n",
        "qa = image1_2_clipped.select('QA_PIXEL')\n",
        "cloud_mask = qa.bitwiseAnd(1 << 0).eq(0)   # bit 0: Fill, keep if 0 (not fill)\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 1).eq(0))  # bit 1: Dilated Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 2).eq(0))  # bit 2: Unused Pixel\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 3).eq(0))  # bit 3: Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 4).eq(0))  # bit 4: Cloud Shadow\n",
        "image1_2_clipped = image1_2_clipped.updateMask(cloud_mask)\n",
        "\n",
        "qa = image1_3_clipped.select('QA_PIXEL')\n",
        "cloud_mask = qa.bitwiseAnd(1 << 0).eq(0)   # bit 0: Fill, keep if 0 (not fill)\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 1).eq(0))  # bit 1: Dilated Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 2).eq(0))  # bit 2: Cirrus\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 3).eq(0))  # bit 3: Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 4).eq(0))  # bit 4: Cloud Shadow\n",
        "image1_3_clipped = image1_3_clipped.updateMask(cloud_mask)\n",
        "\n",
        "qa = image1_4_clipped.select('QA_PIXEL')\n",
        "cloud_mask = qa.bitwiseAnd(1 << 0).eq(0)   # bit 0: Fill, keep if 0 (not fill)\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 1).eq(0))  # bit 1: Dilated Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 2).eq(0))  # bit 2: Cirrus\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 3).eq(0))  # bit 3: Cloud\n",
        "cloud_mask = cloud_mask.And(qa.bitwiseAnd(1 << 4).eq(0))  # bit 4: Cloud Shadow\n",
        "image1_4_clipped = image1_4_clipped.updateMask(cloud_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0zSN28GNLnl"
      },
      "outputs": [],
      "source": [
        "# Defining the masks of all four images as variables in order to combine them into a global mask.\n",
        "mask1 = image1_1_clipped.mask()\n",
        "mask2 = image1_2_clipped.mask()\n",
        "mask3 = image1_3_clipped.mask()\n",
        "mask4 = image1_4_clipped.mask()\n",
        "\n",
        "# Global mask\n",
        "global_mask = mask1.And(mask2).And(mask3).And(mask4)\n",
        "\n",
        "# Applying that global mask to each scene to ensure consistent comparison across all images.\n",
        "\n",
        "image1_1_clipped = image1_1_clipped.updateMask(global_mask)\n",
        "image1_2_clipped = image1_2_clipped.updateMask(global_mask)\n",
        "image1_3_clipped = image1_3_clipped.updateMask(global_mask)\n",
        "image1_4_clipped = image1_4_clipped.updateMask(global_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIc8UxwqiYN5"
      },
      "outputs": [],
      "source": [
        "# Application of a scale factor of 0.0000275 and an offset of −0.2, provided by the Landsat documentation and the normalization of all values on a conceptual scale spanning from 0 to 1.\n",
        "\n",
        "optical_bands = image1_1_clipped.select('SR_B.*').multiply(0.0000275).add(-0.2)\n",
        "\n",
        "image1_1_clipped = image1_1_clipped.addBands(optical_bands, overwrite=True)\n",
        "\n",
        "optical_bands = image1_2_clipped.select('SR_B.*').multiply(0.0000275).add(-0.2)\n",
        "image1_2_clipped = image1_2_clipped.addBands(optical_bands, overwrite=True)\n",
        "\n",
        "optical_bands = image1_3_clipped.select('SR_B.*').multiply(0.0000275).add(-0.2)\n",
        "image1_3_clipped = image1_3_clipped.addBands(optical_bands, overwrite=True)\n",
        "\n",
        "optical_bands = image1_4_clipped.select('SR_B.*').multiply(0.0000275).add(-0.2)\n",
        "image1_4_clipped = image1_4_clipped.addBands(optical_bands, overwrite=True)\n",
        "\n",
        "# Clamping reflectance bands to [0, 1] to remove any slight negatives or >1 values\n",
        "\n",
        "band_names = image1_1_clipped.bandNames().remove('QA_PIXEL').remove('QA_RADSAT')\n",
        "image1 = image1_1_clipped.select(band_names).clamp(0.0, 1.0).addBands(image1_1.select(['QA_PIXEL', 'QA_RADSAT']), overwrite=True)\n",
        "\n",
        "band_names = image1_2_clipped.bandNames().remove('QA_PIXEL').remove('QA_RADSAT')\n",
        "image2 = image1_2_clipped.select(band_names).clamp(0.0, 1.0).addBands(image1_2.select(['QA_PIXEL', 'QA_RADSAT']), overwrite=True)\n",
        "\n",
        "band_names = image1_3_clipped.bandNames().remove('QA_PIXEL').remove('QA_RADSAT')\n",
        "image3 = image1_3_clipped.select(band_names).clamp(0.0, 1.0).addBands(image1_3.select(['QA_PIXEL', 'QA_RADSAT']), overwrite=True)\n",
        "\n",
        "band_names = image1_4_clipped.bandNames().remove('QA_PIXEL').remove('QA_RADSAT')\n",
        "image4 = image1_4_clipped.select(band_names).clamp(0.0, 1.0).addBands(image1_4.select(['QA_PIXEL', 'QA_RADSAT']), overwrite=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXGjhnKhtwye"
      },
      "outputs": [],
      "source": [
        "def classifyImperviousSVM(image,\n",
        "                          catchment,\n",
        "                          bands,\n",
        "                          year_label,\n",
        "                          sample_points,\n",
        "                          seed=999,\n",
        "                          b1_max=None,\n",
        "                          gamma_values=None,\n",
        "                          cost_values=None):\n",
        "    \"\"\"\n",
        "    Train an SVM classifier on 'image' for impervious/v.s. pervious surfaces,\n",
        "    using the GISD30 'b1' truth data.\n",
        "\n",
        "    Args:\n",
        "      image        : ee.Image to classify\n",
        "      catchment    : ee.Feature defining the study area\n",
        "      bands        : list of band names on image to use\n",
        "      year_label   : string, for map & print labels (e.g. '2013')\n",
        "      sample_points: int, how many random reference points to draw\n",
        "      seed         : int, random seed for consistency between runs\n",
        "      b1_max       : int, value 1 for 1984, 4 for 1997, 7 for 2013, 8/None for 2024\n",
        "      gamma_values : list of floats (grid for SVM γ).  Defaults 0.1–1.0\n",
        "      cost_values  : list of ints   (grid for SVM C).  Defaults 1–10\n",
        "\n",
        "    Returns:\n",
        "      dict with keys:\n",
        "        'classifier'      : the final ee.Classifier\n",
        "        'classifiedImage' : ee.Image classification\n",
        "        'accuracy'        : float overall accuracy\n",
        "        'areas_m2'        : {'impervious':…, 'pervious':…, 'total':…}\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Load truth data.\n",
        "    gisd30 = ee.Image(\"projects/sat-io/open-datasets/GISD30_1985_2020\") \\\n",
        "                .select(\"b1\") \\\n",
        "                .unmask(0)\n",
        "\n",
        "    # 2) Define the valid sampling region.\n",
        "    # Reducing the sampling region to areas  not affected by any masking to prevent sample points with no data.\n",
        "    validMask = (image.select(bands)\n",
        "                      .mask()\n",
        "                      .reduce(ee.Reducer.min())\n",
        "                      .toUint8())\n",
        "    validGeom = validMask.reduceToVectors(\n",
        "        geometry=catchment.geometry(),\n",
        "        scale=30,\n",
        "        geometryType='polygon',\n",
        "        eightConnected=False,\n",
        "        labelProperty='mask'\n",
        "    ).geometry()\n",
        "\n",
        "    # 3) Generate random reference points. Multiplying the amount of sample points as defined by the arg sample_points to account for losses from undersampling.\n",
        "    ref_pts = ee.FeatureCollection.randomPoints(\n",
        "        region=validGeom,\n",
        "        points=3 * sample_points,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    n_rand = ref_pts.size().getInfo()\n",
        "    print(f\"{year_label} randomPoints requested = {sample_points}, actual = {n_rand}\")\n",
        "\n",
        "    # 4) Sample GISD30 at those points.\n",
        "    gisd_samp = gisd30.sampleRegions(\n",
        "        collection=ref_pts,\n",
        "        properties=[],\n",
        "        scale=30,\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # 5) Filtering of irrelevant b1 codes if applicable.\n",
        "    if b1_max is not None:\n",
        "        gisd_samp = gisd_samp.filter(ee.Filter.lte('b1', b1_max))\n",
        "\n",
        "    n_gisd = gisd_samp.size().getInfo()\n",
        "    print(f\"{year_label} after sampling GISD30 = {n_gisd}\")\n",
        "\n",
        "    # 6) Turning GISD30 temporal information-including impervious codes into \"1\" based on image year; \"CID\" = 1 if b1>0, else 0.\n",
        "    def _setCID(feat):\n",
        "        return feat.set('CID', ee.Number(feat.get('b1')).gt(0))\n",
        "    gisd_pts = gisd_samp.map(_setCID)\n",
        "\n",
        "    # 7) Adding a random column, sampling the image bands at those truth points.\n",
        "    gisd_pts = gisd_pts.randomColumn('random', seed=seed)\n",
        "    samples = image.select(bands).sampleRegions(\n",
        "        collection=gisd_pts,\n",
        "        properties=['CID','random'],\n",
        "        scale=30,\n",
        "        geometries=True\n",
        "    )\n",
        "\n",
        "    # 8) Creating train/validation dataset.\n",
        "    # 8a) Balancing classes.\n",
        "    perv = samples.filter(ee.Filter.eq('CID', 0))\n",
        "    imp  = samples.filter(ee.Filter.eq('CID', 1))\n",
        "    n_perv = perv.size().getInfo()\n",
        "    n_imp  = imp.size().getInfo()\n",
        "    n_keep = min(n_perv, n_imp)\n",
        "    perv_bal = perv.randomColumn('rand', seed).sort('rand').limit(n_keep)\n",
        "    imp_bal  = imp.randomColumn('rand', seed).sort('rand').limit(n_keep)\n",
        "    balanced = perv_bal.merge(imp_bal)\n",
        "\n",
        "    # 8b) Splitting balanced training dataset into train / validation.\n",
        "    training   = balanced.filter(ee.Filter.lt('random', 0.75))\n",
        "    validation = balanced.filter(ee.Filter.gte('random', 0.75))\n",
        "\n",
        "    print(f\"{year_label} Balanced samples: {n_keep} per class, total {balanced.size().getInfo()}\")\n",
        "\n",
        "    # Calling various information from the train/validation dataset to ensure filtering is working correctly.\n",
        "    n_total       = balanced.size().getInfo()\n",
        "    n_train       = training.size().getInfo()\n",
        "    n_validation  = validation.size().getInfo()\n",
        "    n_pervious    = balanced.filter(ee.Filter.eq('CID', 0)).size().getInfo()\n",
        "    n_impervious  = balanced.filter(ee.Filter.eq('CID', 1)).size().getInfo()\n",
        "\n",
        "    print(f\"{year_label} total samples = {n_total}, \"\n",
        "          f\"training = {n_train}, validation = {n_validation}\")\n",
        "    print(f\"{year_label} pervious samples = {n_pervious}, \"\n",
        "          f\"impervious samples = {n_impervious}\")\n",
        "\n",
        "\n",
        "    # 9) Hyperparameter fine tuning\n",
        "    if gamma_values is None:\n",
        "        gamma_values = [i/10. for i in range(1,11)]\n",
        "    if cost_values is None:\n",
        "        cost_values  = list(range(1,11))\n",
        "\n",
        "    best = {'accuracy': -1}\n",
        "    for γ in gamma_values:\n",
        "        for C in cost_values:\n",
        "            clf = ee.Classifier.libsvm(kernelType='RBF', gamma=γ, cost=C)\n",
        "            trained = clf.train(training, 'CID', bands)\n",
        "            acc = (validation\n",
        "                   .classify(trained)\n",
        "                   .errorMatrix('CID','classification')\n",
        "                   .accuracy()\n",
        "                   .getInfo())\n",
        "            if acc > best['accuracy']:\n",
        "                best = {'gamma': γ, 'cost': C, 'accuracy': acc}\n",
        "\n",
        "    print(f\"{year_label} Best hyperparameters γ={best['gamma']}, C={best['cost']}, \"\n",
        "          f\"Accuracy={best['accuracy']*100:.1f}%\")\n",
        "\n",
        "    # 10) Train the final classifier and then classifying the image.\n",
        "    final_clf = ee.Classifier.libsvm(\n",
        "        kernelType='RBF',\n",
        "        gamma=best['gamma'],\n",
        "        cost= best['cost']\n",
        "    ).train(training, 'CID', bands)\n",
        "\n",
        "    classified = image.select(bands).classify(final_clf)\n",
        "\n",
        "    # 11) Compute area stats\n",
        "    imperv_area = classified.eq(1).multiply(ee.Image.pixelArea())\n",
        "    perv_area   = classified.eq(0).multiply(ee.Image.pixelArea())\n",
        "\n",
        "    sum_imp = imperv_area.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(),\n",
        "        geometry=catchment.geometry(),\n",
        "        scale=30,\n",
        "        maxPixels=1e13\n",
        "    ).get('classification')\n",
        "\n",
        "    sum_per = perv_area.reduceRegion(\n",
        "        reducer=ee.Reducer.sum(),\n",
        "        geometry=catchment.geometry(),\n",
        "        scale=30,\n",
        "        maxPixels=1e13\n",
        "    ).get('classification')\n",
        "\n",
        "    total = catchment.geometry().area()\n",
        "\n",
        "    imp_m2 = sum_imp.getInfo()\n",
        "    per_m2 = sum_per.getInfo()\n",
        "    tot_m2 = total.getInfo()\n",
        "\n",
        "    imp_pct = (imp_m2 / tot_m2) * 100\n",
        "\n",
        "    return {\n",
        "        'classifier':      final_clf,\n",
        "        'classifiedImage': classified,\n",
        "        'validation':     validation,\n",
        "        'training':       training,\n",
        "        'gamma':           best['gamma'],\n",
        "        'cost':            best['cost'],\n",
        "        'accuracy':        best['accuracy'],\n",
        "        'areas_m2': {\n",
        "            'impervious': imp_m2,\n",
        "            'pervious':   per_m2,\n",
        "            'total':      tot_m2\n",
        "        },\n",
        "        'impervious_pct':  imp_pct\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Hu7hhtvNjs"
      },
      "outputs": [],
      "source": [
        "bands_L8 = ['SR_B1','SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7']\n",
        "bands_L5 = ['SR_B1','SR_B2','SR_B3','SR_B4','SR_B5','SR_B7']\n",
        "viz     = {'min':0,'max':1,'palette':['#4caf50','#ff0000']}\n",
        "\n",
        "# 2024 (no b1_max filtering due to using all stages of impervious cover expansion; Landsat 9 with 7 bands)\n",
        "res2024 = classifyImperviousSVM(\n",
        "    image=image4,\n",
        "    catchment=catchment,\n",
        "    bands=bands_L8,\n",
        "    year_label='2024',\n",
        "    sample_points=700,\n",
        "    seed=999,\n",
        "    b1_max=None\n",
        ")\n",
        "\n",
        "# 2013 (filter b1 ≤ 7 = up to the year 2015; Landsat 8 with 7 bands)\n",
        "res2013 = classifyImperviousSVM(\n",
        "    image=image3,\n",
        "    catchment=catchment,\n",
        "    bands=bands_L8,\n",
        "    year_label='2013',\n",
        "    sample_points=700,\n",
        "    seed=999,\n",
        "    b1_max=7\n",
        ")\n",
        "\n",
        "# 1997 (filter b1 ≤ 4 = up to the year 2000; Landsat 5 with 6 bands)\n",
        "res1997 = classifyImperviousSVM(\n",
        "    image=image2,\n",
        "    catchment=catchment,\n",
        "    bands=bands_L5,\n",
        "    year_label='1997',\n",
        "    sample_points=600,\n",
        "    seed=999,\n",
        "    b1_max=4\n",
        ")\n",
        "\n",
        "# 1984 (filter b1 = 1, only including pre-1985 impervious surfaces; Landsat 5 with 6 bands)\n",
        "res1984 = classifyImperviousSVM(\n",
        "    image=image1,\n",
        "    catchment=catchment,\n",
        "    bands=bands_L5,\n",
        "    year_label='1984',\n",
        "    sample_points=600,\n",
        "    seed=999,\n",
        "    b1_max=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4FKyUNFs7iP"
      },
      "outputs": [],
      "source": [
        "# Print % impervious surface cover.\n",
        "print(f\"1984 impervious surface percentage = {res1984['impervious_pct']:.2f}% of catchment\")\n",
        "print(f\"1997 impervious surface percentage = {res1997['impervious_pct']:.2f}% of catchment\")\n",
        "print(f\"2013 impervious surface percentage = {res2013['impervious_pct']:.2f}% of catchment\")\n",
        "print(f\"2024 impervious surface percentage = {res2024['impervious_pct']:.2f}% of catchment\")\n",
        "\n",
        "# Print impervious surface cover in absolute numbers\n",
        "print(f\"1984 impervious surface area = {res1984['areas_m2']['impervious']/1e6:.2f} km²\")\n",
        "print(f\"1997 impervious surface area = {res1997['areas_m2']['impervious']/1e6:.2f} km²\")\n",
        "print(f\"2013 impervious surface area = {res2013['areas_m2']['impervious']/1e6:.2f} km²\")\n",
        "print(f\"2024 impervious surface area = {res2024['areas_m2']['impervious']/1e6:.2f} km²\")\n",
        "\n",
        "print(f\"1984 impervious surface area = {res1984['areas_m2']['total']/1e6:.2f} km²\")\n",
        "print(f\"1997 impervious surface area = {res1997['areas_m2']['total']/1e6:.2f} km²\")\n",
        "print(f\"2013 impervious surface area = {res2013['areas_m2']['total']/1e6:.2f} km²\")\n",
        "print(f\"2024 impervious surface area = {res2024['areas_m2']['total']/1e6:.2f} km²\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzL2mk0K2QDR"
      },
      "outputs": [],
      "source": [
        "# Creating a map centered on the catchment.\n",
        "center = catchment.geometry().centroid().coordinates().getInfo()  # [lon, lat]\n",
        "m = geemap.Map(center=map_center, zoom=12)\n",
        "\n",
        "# Visual parameters.\n",
        "viz = {'min': 0, 'max': 1, 'palette': ['#c8d9c5', '#ff0000']}\n",
        "\n",
        "# Adding each classified result as its own layer.\n",
        "m.addLayer(res1984['classifiedImage'], viz, 'Impervious 1984')\n",
        "m.addLayer(res1997['classifiedImage'], viz, 'Impervious 1997')\n",
        "m.addLayer(res2013['classifiedImage'], viz, 'Impervious 2013')\n",
        "m.addLayer(res2024['classifiedImage'], viz, 'Impervious 2024')\n",
        "m.addLayerControl()\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjNrAnT8C5yr"
      },
      "outputs": [],
      "source": [
        "# Calculating Change Detection (gain, loss, and net) across all three time ranges with at least 2 steps.\n",
        "\n",
        "# Defining total area and pixel area for later use.\n",
        "total_m2  = catchment.geometry().area().getInfo()      # whole catchment\n",
        "total_km2 = total_m2 / 1e6                             # just for context\n",
        "pixel_area = ee.Image.pixelArea()                      # re-use later\n",
        "\n",
        "# From 1984 to 20204\n",
        "# Defining pixel value +1 = gain; -1 = loss.\n",
        "change_84_24 = res2024['classifiedImage'].subtract(res1984['classifiedImage'])\n",
        "gain_84_24   = change_84_24.eq(1)\n",
        "loss_84_24   = change_84_24.eq(-1)\n",
        "\n",
        "# Calculating first gain and then loss areas (in square meters).\n",
        "gain_m2_84_24 = gain_84_24.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "loss_m2_84_24 = loss_84_24.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "# Converting calculation results to square kilometers.\n",
        "gain_km2_84_24 = gain_m2_84_24 / 1e6\n",
        "loss_km2_84_24 = loss_m2_84_24 / 1e6\n",
        "net_km2_84_24  = (gain_m2_84_24 - loss_m2_84_24) / 1e6\n",
        "\n",
        "# Converting calculation results to a percentage point increase/decrease.\n",
        "gain_pct_84_24 = (gain_m2_84_24 / total_m2) * 100\n",
        "loss_pct_84_24 = (loss_m2_84_24 / total_m2) * 100\n",
        "net_pct_84_24  = ((gain_m2_84_24 - loss_m2_84_24) / total_m2) * 100\n",
        "\n",
        "print(f\"1984-2024: Gain {gain_km2_84_24:.2f} km2 ({gain_pct_84_24:.2f} %), \"\n",
        "      f\"Loss {loss_km2_84_24:.2f} km2 (-{loss_pct_84_24:.2f} %), \"\n",
        "      f\"Net {net_km2_84_24:.2f} km2 ({net_pct_84_24:.2f} %)\")\n",
        "\n",
        "\n",
        "#####\n",
        "# Same process for 1984 to 2013.\n",
        "change_84_13 = res2013['classifiedImage'].subtract(res1984['classifiedImage'])\n",
        "gain_84_13   = change_84_13.eq(1)\n",
        "loss_84_13   = change_84_13.eq(-1)\n",
        "\n",
        "gain_m2_84_13 = gain_84_13.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "loss_m2_84_13 = loss_84_13.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "gain_km2_84_13 = gain_m2_84_13 / 1e6\n",
        "loss_km2_84_13 = loss_m2_84_13 / 1e6\n",
        "net_km2_84_13  = (gain_m2_84_13 - loss_m2_84_13) / 1e6\n",
        "\n",
        "gain_pct_84_13 = (gain_m2_84_13 / total_m2) * 100\n",
        "loss_pct_84_13 = (loss_m2_84_13 / total_m2) * 100\n",
        "net_pct_84_13  = ((gain_m2_84_13 - loss_m2_84_13) / total_m2) * 100\n",
        "\n",
        "print(f\"1984-2013: Gain {gain_km2_84_13:.2f} km2 ({gain_pct_84_13:.2f} %), \"\n",
        "      f\"Loss {loss_km2_84_13:.2f} km2 (-{loss_pct_84_13:.2f} %), \"\n",
        "      f\"Net {net_km2_84_13:.2f} km2 ({net_pct_84_13:.2f} %)\")\n",
        "\n",
        "\n",
        "#####\n",
        "# And for 1997 to 2024.\n",
        "change_97_24 = res2024['classifiedImage'].subtract(res1997['classifiedImage'])\n",
        "gain_97_24   = change_97_24.eq(1)\n",
        "loss_97_24   = change_97_24.eq(-1)\n",
        "\n",
        "gain_m2_97_24 = gain_97_24.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "loss_m2_97_24 = loss_97_24.multiply(pixel_area) \\\n",
        "    .reduceRegion(ee.Reducer.sum(),\n",
        "                  geometry=catchment.geometry(),\n",
        "                  scale=30,\n",
        "                  maxPixels=1e13) \\\n",
        "    .get('classification').getInfo()\n",
        "\n",
        "gain_km2_97_24 = gain_m2_97_24 / 1e6\n",
        "loss_km2_97_24 = loss_m2_97_24 / 1e6\n",
        "net_km2_97_24  = (gain_m2_97_24 - loss_m2_97_24) / 1e6\n",
        "\n",
        "gain_pct_97_24 = (gain_m2_97_24 / total_m2) * 100\n",
        "loss_pct_97_24 = (loss_m2_97_24 / total_m2) * 100\n",
        "net_pct_97_24  = ((gain_m2_97_24 - loss_m2_97_24) / total_m2) * 100\n",
        "\n",
        "print(f\"1997-2024: Gain {gain_km2_97_24:.2f} km2 ({gain_pct_97_24:.2f} %), \"\n",
        "      f\"Loss {loss_km2_97_24:.2f} km2 (-{loss_pct_97_24:.2f} %), \"\n",
        "      f\"Net {net_km2_97_24:.2f} km2 ({net_pct_97_24:.2f} %)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8D0qercBg8L"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix-based validation using Overall Accuracy, Producer's Accuracy,\n",
        "# User's Accuarcy, Kappa Coefficient, and F1-Score.\n",
        "\n",
        "# image1 (1984)\n",
        "validated1 = res1984['validation'].classify(res1984['classifier'])\n",
        "\n",
        "confmatrix1 = validated1.errorMatrix('CID', 'classification')\n",
        "\n",
        "print('1984 Confusion Matrix', confmatrix1.getInfo())\n",
        "\n",
        "print('1984 Overall Accuracy:', confmatrix1.accuracy().getInfo())\n",
        "print('1984 Producers Accuracy:', confmatrix1.producersAccuracy().getInfo())\n",
        "print('1984 Users Accuracy:', confmatrix1.consumersAccuracy().getInfo())\n",
        "print('1984 Kappa Coefficient:', confmatrix1.kappa().getInfo())\n",
        "print('1984 F1-Score:', confmatrix1.fscore(1).getInfo())\n",
        "\n",
        "# image2 (1997)\n",
        "validated2 = res1997['validation'].classify(res1997['classifier'])\n",
        "\n",
        "confmatrix2 = validated2.errorMatrix('CID', 'classification')\n",
        "\n",
        "print('1997 Confusion Matrix', confmatrix2.getInfo())\n",
        "\n",
        "print('1997 Overall Accuracy:', confmatrix2.accuracy().getInfo())\n",
        "print('1997 Producers Accuracy:', confmatrix2.producersAccuracy().getInfo())\n",
        "print('1997 Users Accuracy:', confmatrix2.consumersAccuracy().getInfo())\n",
        "print('1997 Kappa Coefficient:', confmatrix2.kappa().getInfo())\n",
        "print('1997 F1-Score:', confmatrix2.fscore(1).getInfo())\n",
        "\n",
        "# image3 (2013)\n",
        "validated3 = res2013['validation'].classify(res2013['classifier'])\n",
        "\n",
        "confmatrix3 = validated3.errorMatrix('CID', 'classification')\n",
        "\n",
        "print('2013 Confusion Matrix', confmatrix3.getInfo())\n",
        "\n",
        "print('2013 Overall Accuracy:', confmatrix3.accuracy().getInfo())\n",
        "print('2013 Producers Accuracy:', confmatrix3.producersAccuracy().getInfo())\n",
        "print('2013 Users Accuracy:', confmatrix3.consumersAccuracy().getInfo())\n",
        "print('2013 Kappa Coefficient:', confmatrix3.kappa().getInfo())\n",
        "print('2013 F1-Score:', confmatrix3.fscore(1).getInfo())\n",
        "\n",
        "# image4 (2024)\n",
        "validated4 = res2024['validation'].classify(res2024['classifier'])\n",
        "\n",
        "confmatrix4 = validated4.errorMatrix('CID', 'classification')\n",
        "\n",
        "print('2024 Confusion Matrix', confmatrix4.getInfo())\n",
        "\n",
        "print('2024 Overall Accuracy:', confmatrix4.accuracy().getInfo())\n",
        "print('2024 Producers Accuracy:', confmatrix4.producersAccuracy().getInfo())\n",
        "print('2024 Users Accuracy:', confmatrix4.consumersAccuracy().getInfo())\n",
        "print('2024 Kappa Coefficient:', confmatrix4.kappa().getInfo())\n",
        "print('2024 F1-Score:', confmatrix4.fscore(1).getInfo())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGzSMExzukZEMe3z44A957",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}